{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Comparison\n",
    "\n",
    "Julia is also often cited as a speedy alternative to Python and Numba. \n",
    "Unlike Python, jit-compilation is built into the language, so there is no need to use a separate library.\n",
    "It is often easier to design large-scale projects in Julia than in numba thanks to this deeper integration of jit-compilation into the language.\n",
    "\n",
    "Here we will compare the performance of Julia to Python and Numba for the same problem as in our previous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `integrate_context`\n",
    "The function `integrate_context` uses a hidden function `calc_rho` to integrate contextual input into a context vector. \n",
    "Here we implement it using Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "integrate_context (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function calc_rho(cdot, B)\n",
    "    rho = sqrt(1 + (B * B) * ((cdot * cdot) - 1)) - (B * cdot)\n",
    "    return rho\n",
    "end\n",
    "\n",
    "function integrate_context(c, c_in, B, c_ind)\n",
    "    cdot = 0\n",
    "    for i in c_ind[1]:c_ind[2]\n",
    "        cdot += c[i] * c_in[i]\n",
    "    end\n",
    "    rho = calc_rho(cdot, B)\n",
    "    \n",
    "    for i in c_ind[1]:c_ind[2]\n",
    "        c[i] = rho * c[i] + B * c_in[i]\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = [0.0, 0.09128709, 0.18257419, 0.27386128, 0.36514837, 0.8660254]\n",
    "c2 = [0.15655607, 0.24875946, 0.34096284, 0.43316622, 0.52536961, 0.57767384]\n",
    "B = 0.5\n",
    "c_ind = [1, 6]  # Julia arrays are 1-indexed\n",
    "\n",
    "integrate_context(c1, c2, B, c_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 991 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m41.574 ns\u001b[22m\u001b[39m … \u001b[35m 1.203 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m42.079 ns              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m46.847 ns\u001b[22m\u001b[39m ± \u001b[32m18.320 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m█\u001b[34m▄\u001b[39m\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[32m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▃\u001b[39m▂\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▂\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  41.6 ns\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        95 ns \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@benchmark integrate_context(c1, c2, B, c_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our evaluation of the performance of `integrate_context` as implemented using numba, we found an average runtime of 556 ns +/- 14.2 ns per loop. \n",
    "\n",
    "Here, using Julia, we find an average runtime of 46.8 ns +/- 18.3 ns per loop.\n",
    "\n",
    "This is over 10 times faster than the numba implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
